{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading words: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1076 is causing problems\n",
      "row 2075 is causing problems\n",
      "row 2533 is causing problems\n",
      "row 3573 is causing problems\n",
      "row 4250 is causing problems\n",
      "row 4380 is causing problems\n",
      "row 4725 is causing problems\n",
      "row 4730 is causing problems\n",
      "row 5592 is causing problems\n",
      "row 5895 is causing problems\n",
      "row 6149 is causing problems\n",
      "row 6816 is causing problems\n",
      "row 7510 is causing problems\n",
      "row 7535 is causing problems\n",
      "499 have been removed.\n",
      "8424 songs remain in the dataset.\n",
      "stop words ['i', 'me', 'my', 'myself', 'we'] ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lyric Data Processing\n",
    "CMPE 351 Group Project\n",
    "Spring 2021\n",
    "\"\"\"\n",
    "#bin names for genres rock, country, hip hop, pop\n",
    "\n",
    "#%% Import actual data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ld = pd.read_csv('./data/track_features.csv')\n",
    "ld = ld[ld[\"lyrics\"]!=\"''\"]\n",
    "\n",
    "#%% Encode labels as 0 or 1\n",
    "\n",
    "ld.valence = round(ld.valence)\n",
    "ld.danceability = round(ld.danceability)\n",
    "\n",
    "#%% Language filter\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('words')\n",
    "def eng_ratio(text):\n",
    "    ''' Returns the ratio of non-English to English words from a text '''\n",
    "\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words()) \n",
    "    text_vocab = set(w.lower() for w in text.split() if w.lower().isalpha()) \n",
    "    unusual = text_vocab.difference(english_vocab)\n",
    "    diff = len(unusual)/len(text_vocab)\n",
    "    return diff\n",
    "\n",
    "\n",
    "before = ld.shape[0]\n",
    "for row_id in ld.index:\n",
    "    text = ld.loc[row_id]['lyrics']\n",
    "    try:\n",
    "        diff = eng_ratio(text)\n",
    "    except:\n",
    "        ld = ld[ld.index != row_id]\n",
    "        print('row %s is causing problems' %row_id)\n",
    "    if diff >= 0.5:\n",
    "        ld = ld[ld.index != row_id]\n",
    "after = ld.shape[0]\n",
    "rem = before - after\n",
    "print('%s have been removed.' %rem)\n",
    "print('%s songs remain in the dataset.' %after)\n",
    "\n",
    "dataPath1 = \"/Users/Ryan/Documents/GitHub/351-lyric-analysis/data/filtered_data.csv\"\n",
    "\n",
    "# ld.to_csv(os.path.join(dataPath1), index=False)\n",
    "\n",
    "#%% Split into training, test\n",
    "import numpy as np\n",
    "\n",
    "msk = np.random.rand(len(ld)) < 0.8\n",
    "\n",
    "train = ld[msk]\n",
    "test = ld[~msk]\n",
    "                 \n",
    "\n",
    "#%% Porter-Stemmer Tokenizer, suffix stripper\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
    "    \"\"\"\n",
    "    A Porter-Stemmer-Tokenizer hybrid to splits sentences into words (tokens) \n",
    "    and applies the porter stemming algorithm to each of the obtained token. \n",
    "    Tokens that are only consisting of punctuation characters are removed as well.\n",
    "    Only tokens that consist of more than one letter are being kept.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        \n",
    "    text : `str`. \n",
    "      A sentence that is to split into words.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    no_punct : `str`. \n",
    "      A list of tokens after stemming and removing Sentence punctuation patterns.\n",
    "    \n",
    "    \"\"\"\n",
    "    lower_txt = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(lower_txt)\n",
    "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
    "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return no_punct\n",
    "\n",
    "#%% Stop words\n",
    "\n",
    "# # One-time download of stop words file:\n",
    "# nltk.download('stopwords')\n",
    "# stp = nltk.corpus.stopwords.words('english')\n",
    "# with open('./stopwords_eng.txt', 'w') as outfile:\n",
    "#     outfile.write('\\n'.join(stp))\n",
    "    \n",
    "    \n",
    "with open('./stopwords_eng.txt', 'r') as infile:\n",
    "    stop_words = infile.read().splitlines()\n",
    "print('stop words %s ...' %stop_words[:5])\n",
    "\n",
    "#%% Count Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# can try different values for ngram_range\n",
    "countVec = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer,\n",
    "            ngram_range=(1,1)\n",
    "    )\n",
    "\n",
    "valenceTrain = train[\"valence\"]\n",
    "valenceTest = test[\"valence\"]\n",
    "danceTrain = train[\"danceability\"]\n",
    "danceTest = test[\"danceability\"]\n",
    "# print('Vocabulary size: %s' %len(countVecTrain.get_feature_names()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c68dda4c4268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Beginning of valence prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvalenceTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"valence\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvalenceTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"valence\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdanceTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"danceability\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdanceTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"danceability\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#non genre specific\n",
    "valenceTrain = train[\"valence\"].astype(str)\n",
    "valenceTest = test[\"valence\"].astype(str)\n",
    "danceTrain = train[\"danceability\"].astype(str)\n",
    "danceTest = test[\"danceability\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec.fit(train[\"lyrics\"].values.ravel())\n",
    "countVecTrain = countVec.transform(train[\"lyrics\"].values)\n",
    "countVecTest = countVec.transform(test[\"lyrics\"].values) \n",
    "#end of non genre specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre specific\n",
    "train_rock = train[train['genre']==\"rock\"]\n",
    "train_pop = train[train['genre']==\"pop\"]\n",
    "train_hiphop = train[train['genre']==\"hip hop\"]\n",
    "train_country = train[train['genre']==\"country\"]\n",
    "\n",
    "test_rock = test[test['genre']==\"rock\"]\n",
    "test_pop = test[test['genre']==\"pop\"]\n",
    "test_hiphop = test[test['genre']==\"hip hop\"]\n",
    "test_country = test[test['genre']==\"country\"]\n",
    "\n",
    "#For the next sections, choose only one depending on the genre you want to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre specific country\n",
    "valenceTrain = train_country[\"valence\"].astype(str)\n",
    "valenceTest = test_country[\"valence\"].astype(str)\n",
    "danceTrain = train_country[\"danceability\"].astype(str)\n",
    "danceTest = test_country[\"danceability\"].astype(str)\n",
    "\n",
    "countVec.fit(train_country[\"lyrics\"].values.ravel())\n",
    "countVecTrain = countVec.transform(train_country[\"lyrics\"].values)\n",
    "countVecTest = countVec.transform(test_country[\"lyrics\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre specific pop\n",
    "valenceTrain = train_pop[\"valence\"].astype(str)\n",
    "valenceTest = test_pop[\"valence\"].astype(str)\n",
    "danceTrain = train_pop[\"danceability\"].astype(str)\n",
    "danceTest = test_pop[\"danceability\"].astype(str)\n",
    "\n",
    "countVec.fit(train_pop[\"lyrics\"].values.ravel())\n",
    "countVecTrain = countVec.transform(train_pop[\"lyrics\"].values)\n",
    "countVecTest = countVec.transform(test_pop[\"lyrics\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre specific hiphop\n",
    "valenceTrain = train_hiphop[\"valence\"].astype(str)\n",
    "valenceTest = test_hiphop[\"valence\"].astype(str)\n",
    "danceTrain = train_hiphop[\"danceability\"].astype(str)\n",
    "danceTest = test_hiphop[\"danceability\"].astype(str)\n",
    "\n",
    "countVec.fit(train_hiphop[\"lyrics\"].values.ravel())\n",
    "countVecTrain = countVec.transform(train_hiphop[\"lyrics\"].values)\n",
    "countVecTest = countVec.transform(test_hiphop[\"lyrics\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre specific rock\n",
    "valenceTrain = train_rock[\"valence\"].astype(str)\n",
    "valenceTest = test_rock[\"valence\"].astype(str)\n",
    "danceTrain = train_rock[\"danceability\"].astype(str)\n",
    "danceTest = test_rock[\"danceability\"].astype(str)\n",
    "\n",
    "countVec.fit(train_rock[\"lyrics\"].values.ravel())\n",
    "countVecTrain = countVec.transform(train_rock[\"lyrics\"].values)\n",
    "countVecTest = countVec.transform(test_rock[\"lyrics\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end of genre specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Valence Accuracy score: 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Model valence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(countVecTrain, valenceTrain)\n",
    "NB_V_predictions = clf_NB.predict(countVecTest)\n",
    "print('NB Valence Accuracy score:' , accuracy_score(valenceTest, NB_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Valence Accuracy: 0.6041666666666666\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Model valence\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf_RF=RandomForestClassifier(n_estimators=10)\n",
    "clf_RF.fit(countVecTrain,valenceTrain)\n",
    "RF_V_predictions = clf_RF.predict(countVecTest)\n",
    "print(\"RF Valence Accuracy:\", accuracy_score(valenceTest, RF_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Valence Accuracy: 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression valence\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_LR = LogisticRegression()\n",
    "clf_LR.fit(countVecTrain, valenceTrain)\n",
    "LR_V_predictions = clf_LR.predict(countVecTest)\n",
    "print(\"LR Valence Accuracy:\", accuracy_score(valenceTest, LR_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JTOCo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JTOCo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Start of CNN valence\n",
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('filtered_data.csv')\n",
    "df1 = df[['lyrics', 'valence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tokens\n"
     ]
    }
   ],
   "source": [
    "#df1.valence = pd.cut(df1.valence,bins=[0,0.5,1],labels=[0,1])\n",
    "df1.lyrics = df1.lyrics.replace(r'\\\\n',' ', regex=True) \n",
    "df1.lyrics = df1['lyrics'].astype(str)\n",
    "\n",
    "print(\"getting tokens\")\n",
    "\n",
    "#Tokens\n",
    "tokens = [word_tokenize(sen) for sen in df1.lyrics]\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#Pos and Neg\n",
    "pos = []\n",
    "neg = []\n",
    "for l in df1.valence:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "df1['Pos']= pos\n",
    "df1['Neg']= neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>valence</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>'Once upon a time you dressed so fine Threw th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"Load up on guns, bring your friends It's fun ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>'CDsAC/DC - Back in Black (1980) - For Those A...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>'I, I love the colorful clothes she wears And ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>'Deep down in Louisiana close to New Orleans W...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  valence  Pos  Neg\n",
       "0  'Once upon a time you dressed so fine Threw th...      1.0    1    0\n",
       "1  \"Load up on guns, bring your friends It's fun ...      1.0    1    0\n",
       "2  'CDsAC/DC - Back in Black (1980) - For Those A...      0.0    0    1\n",
       "3  'I, I love the colorful clothes she wears And ...      0.0    0    1\n",
       "4  'Deep down in Louisiana close to New Orleans W...      1.0    1    0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "def removeStopWords(tokens): \n",
    "    return [w for w in tokens if w not in stoplist]\n",
    "filtered_words = [removeStopWords(i) for i in lower_tokens]\n",
    "df1['lyrics'] = [' '.join(i) for i in filtered_words]\n",
    "df1['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15707217 words total, with a vocabulary size of 165789\n",
      "Max sentence length is 146616\n",
      "1481629 words total, with a vocabulary size of 72671\n",
      "Max sentence length is 98008\n"
     ]
    }
   ],
   "source": [
    "data = df1[['lyrics', 'tokens', 'valence', 'Pos', 'Neg']]\n",
    "data.head()\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "all_test_words = [word for tokens in data_test['tokens'] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test['tokens']]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print('%s words total, with a vocabulary size of %s' % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print('Max sentence length is %s' % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MS_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"lyrics\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"lyrics\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124932 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MS_LENGTH)\n",
    "\n",
    "tew = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    tew[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    \n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"lyrics\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MS_LENGTH)\n",
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values\n",
    "\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 300)      37479900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 49, 250)      150250      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 48, 250)      225250      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 47, 250)      300250      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 46, 250)      375250      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 45, 250)      450250      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 250)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 250)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 250)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 250)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 250)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1250)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1250)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          160128      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,141,536\n",
      "Trainable params: 1,661,636\n",
      "Non-trainable params: 37,479,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_layer = Embedding(len(train_word_index)+1,\n",
    "                         300,\n",
    "                         weights=[tew],\n",
    "                         input_length=MS_LENGTH,\n",
    "                          trainable=False)\n",
    "    \n",
    "sequence_input = Input(shape=(MS_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "epochs_count = 3\n",
    "b_size = 36\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=250, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "\n",
    "lm = concatenate(convs, axis=1)\n",
    "\n",
    "x = Dropout(0.1)(lm)  \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(len(list(label_names)), activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "190/190 [==============================] - 22s 113ms/step - loss: 0.1230 - acc: 0.9414 - val_loss: 1.6374 - val_acc: 0.6285\n",
      "Epoch 2/3\n",
      "190/190 [==============================] - 22s 113ms/step - loss: 0.0967 - acc: 0.9575 - val_loss: 1.3337 - val_acc: 0.6087\n",
      "Epoch 3/3\n",
      "190/190 [==============================] - 22s 116ms/step - loss: 0.0886 - acc: 0.9587 - val_loss: 1.6878 - val_acc: 0.6443\n",
      "1/1 [==============================] - 1s 897ms/step\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=epochs_count, validation_split=0.1, shuffle=True, batch_size=b_size)\n",
    "\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "labels = [1, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.594306049822064"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "sum(data_test.valence==prediction_labels)/len(prediction_labels)\n",
    "#end of CNN valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833333333333334"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble for Max voting for several models valence\n",
    "\"\"\"\n",
    "Addtional Models - SVM, DT\n",
    " \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "svc = SVC(kernel='poly, probability=True)\n",
    "dt = DecisionTreeClassifier()\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "from sklearn.ensemble import VotingClassifier\n",
    " \n",
    "classifiers = [('lr', clf_LR), ('nb', clf_NB), ('rf', clf_RF)]\n",
    "vc = VotingClassifier(estimators=classifiers, voting='hard')\n",
    " \n",
    "# One method\n",
    "# from - https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ \n",
    "model = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "model.fit(countVecTrain, valenceTrain)\n",
    "model.score(countVecTest, valenceTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56920692 0.579937   0.54366981 0.58467601]\n"
     ]
    }
   ],
   "source": [
    "# Another method  #DELETE DELETE DELETE\n",
    "# Taken from - https://medium.com/@sanchitamangale12/voting-classifier-1be10db6d7a5 \n",
    "from sklearn.model_selection import cross_val_score\n",
    "a = []\n",
    "a.append(cross_val_score(lr, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    "a.append(cross_val_score(nb, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    "a.append(cross_val_score(rf, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    "a.append(cross_val_score(vc, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    " \n",
    "import numpy as np\n",
    "print(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things for CNN #DELETE DELETE DELETE\n",
    "\n",
    "\"\"\"\n",
    "# Simple Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# replace line of code that\n",
    "# model = Model(sequence_input, preds)\n",
    "model = BaggingClassifier(Model(sequence_input, preds))\n",
    "\n",
    "# AdaBoostClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(Model(sequence_input, preds))\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "CNN Model - Average voting ensemble\n",
    "Adapted from https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "Josh's CNN model is used down below\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "# define Model + fit on dataset\n",
    "def fit_model(x_train, y_tr):\n",
    "    embedding_layer = Embedding(len(train_word_index)+1, 300, weights=[tew], input_length=MS_LENGTH, trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(MS_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    epochs_count = 5\n",
    "    b_size = 36\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=250, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    lm = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(lm)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(len(list(label_names)), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.fit(x_train, y_tr, epochs=epochs_count, validation_split=0.1, shuffle=True, batch_size=b_size)\n",
    "    return model\n",
    "\n",
    "# Ensemble prediction\n",
    "def ensemble_predictions(models, x_test):\n",
    "    # Make predictions\n",
    "    yhats = [model.predict(x_test) for model in models]\n",
    "    yhats = array(yhats)\n",
    "    # Sum ensemble members\n",
    "    model_sum = numpy.sum(yhats, axis=0)\n",
    "    result = argmax(model_sum, axis=1)\n",
    "    return result\n",
    "\n",
    "# Evaluate specifics\n",
    "# Double check names of testing/training data\n",
    "def evaluate_n_models(models, n_models, x_test, y_test):\n",
    "    subset = models[:n_models]\n",
    "    # Make a predicition\n",
    "    yhat = ensemble_predictions(subset, x_test)\n",
    "    return accuracy_score(test_y, yhat)\n",
    "\n",
    "n_models = 5\n",
    "models = [fit_model(train_x, train_y) for _ in range(n_models)]\n",
    "\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(models)+1):\n",
    "\t# evaluate model with i members\n",
    "\tensemble_score = evaluate_n_models(members, i, test_x, test_y)\n",
    "\t# evaluate the i'th model standalone\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, single_score = members[i-1].evaluate(test_x, testy_enc, verbose=0)\n",
    "\t# summarize this step\n",
    "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "\tensemble_scores.append(ensemble_score)\n",
    "\tsingle_scores.append(single_score)\n",
    "\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(models)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning of danceability prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Danceability Accuracy score: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Model  danceability\n",
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(countVecTrain, danceTrain)\n",
    "NB_D_predictions = clf_NB.predict(countVecTest)\n",
    "print('NB Danceability Accuracy score:' , accuracy_score(danceTest, NB_D_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Danceability Accuracy: 0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Model danceability\n",
    "clf_RF=RandomForestClassifier(n_estimators=10)\n",
    "clf_RF.fit(countVecTrain,danceTrain)\n",
    "RF_D_predictions = clf_RF.predict(countVecTest)\n",
    "print(\"RF Danceability Accuracy:\", accuracy_score(danceTest, RF_D_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Danceability Accuracy: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression danceability\n",
    "clf_LR = LogisticRegression()\n",
    "clf_LR.fit(countVecTrain, danceTrain)\n",
    "LR_D_predictions = clf_LR.predict(countVecTest)\n",
    "print(\"LR Danceability Accuracy:\", accuracy_score(danceTest, LR_D_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6041666666666666"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble for Max voting for several models danceability\n",
    "\"\"\"\n",
    "Addtional Models - SVM, DT\n",
    " \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "svc = SVC(kernel='poly, probability=True)\n",
    "dt = DecisionTreeClassifier()\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "from sklearn.ensemble import VotingClassifier\n",
    " \n",
    "classifiers = [('lr', clf_LR), ('nb', clf_NB), ('rf', clf_RF)]\n",
    "vc = VotingClassifier(estimators=classifiers, voting='hard')\n",
    " \n",
    "# One method\n",
    "# from - https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ \n",
    "model = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "model.fit(countVecTrain, danceTrain)\n",
    "model.score(countVecTest, danceTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN for danceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('filtered_data.csv')\n",
    "df1 = df[['lyrics', 'danceability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tokens\n"
     ]
    }
   ],
   "source": [
    "#df1.valence = pd.cut(df1.valence,bins=[0,0.5,1],labels=[0,1])\n",
    "df1.lyrics = df1.lyrics.replace(r'\\\\n',' ', regex=True) \n",
    "df1.lyrics = df1['lyrics'].astype(str)\n",
    "\n",
    "print(\"getting tokens\")\n",
    "\n",
    "#Tokens\n",
    "tokens = [word_tokenize(sen) for sen in df1.lyrics]\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#Pos and Neg\n",
    "pos = []\n",
    "neg = []\n",
    "for l in df1.danceability:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "df1['Pos']= pos\n",
    "df1['Neg']= neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "def removeStopWords(tokens): \n",
    "    return [w for w in tokens if w not in stoplist]\n",
    "filtered_words = [removeStopWords(i) for i in lower_tokens]\n",
    "df1['lyrics'] = [' '.join(i) for i in filtered_words]\n",
    "df1['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15707217 words total, with a vocabulary size of 165789\n",
      "Max sentence length is 146616\n",
      "1481629 words total, with a vocabulary size of 72671\n",
      "Max sentence length is 98008\n"
     ]
    }
   ],
   "source": [
    "data = df1[['lyrics', 'tokens', 'danceability', 'Pos', 'Neg']]\n",
    "data.head()\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "all_test_words = [word for tokens in data_test['tokens'] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test['tokens']]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print('%s words total, with a vocabulary size of %s' % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print('Max sentence length is %s' % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MS_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"lyrics\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"lyrics\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124932 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MS_LENGTH)\n",
    "\n",
    "tew = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    tew[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    \n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"lyrics\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MS_LENGTH)\n",
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values\n",
    "\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 50, 300)      37479900    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 49, 250)      150250      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 48, 250)      225250      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 47, 250)      300250      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 46, 250)      375250      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 45, 250)      450250      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 250)          0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 250)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 250)          0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 250)          0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 250)          0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1250)         0           global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1250)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          160128      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            258         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,141,536\n",
      "Trainable params: 1,661,636\n",
      "Non-trainable params: 37,479,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_layer = Embedding(len(train_word_index)+1,\n",
    "                         300,\n",
    "                         weights=[tew],\n",
    "                         input_length=MS_LENGTH,\n",
    "                          trainable=False)\n",
    "    \n",
    "sequence_input = Input(shape=(MS_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "epochs_count = 10\n",
    "b_size = 36\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=250, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "\n",
    "lm = concatenate(convs, axis=1)\n",
    "\n",
    "x = Dropout(0.1)(lm)  \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(len(list(label_names)), activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "190/190 [==============================] - 35s 164ms/step - loss: 0.7066 - acc: 0.6448 - val_loss: 0.6111 - val_acc: 0.6759\n",
      "Epoch 2/10\n",
      "190/190 [==============================] - 28s 150ms/step - loss: 0.6267 - acc: 0.6649 - val_loss: 0.5945 - val_acc: 0.6706\n",
      "Epoch 3/10\n",
      "190/190 [==============================] - 27s 140ms/step - loss: 0.5902 - acc: 0.6746 - val_loss: 0.6035 - val_acc: 0.6825\n",
      "Epoch 4/10\n",
      "190/190 [==============================] - 23s 122ms/step - loss: 0.5330 - acc: 0.7179 - val_loss: 0.6486 - val_acc: 0.6785\n",
      "Epoch 5/10\n",
      "190/190 [==============================] - 23s 119ms/step - loss: 0.4209 - acc: 0.7969 - val_loss: 0.6880 - val_acc: 0.6535\n",
      "Epoch 6/10\n",
      "190/190 [==============================] - 23s 119ms/step - loss: 0.2539 - acc: 0.9024 - val_loss: 0.8892 - val_acc: 0.6509\n",
      "Epoch 7/10\n",
      "190/190 [==============================] - 23s 119ms/step - loss: 0.1728 - acc: 0.9321 - val_loss: 0.9226 - val_acc: 0.6522\n",
      "Epoch 8/10\n",
      "190/190 [==============================] - 22s 117ms/step - loss: 0.1327 - acc: 0.9418 - val_loss: 1.0015 - val_acc: 0.6430\n",
      "Epoch 9/10\n",
      "190/190 [==============================] - 23s 120ms/step - loss: 0.1060 - acc: 0.9504 - val_loss: 1.2944 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "190/190 [==============================] - 22s 118ms/step - loss: 0.0990 - acc: 0.9442 - val_loss: 1.1757 - val_acc: 0.6153\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=epochs_count, validation_split=0.1, shuffle=True, batch_size=b_size)\n",
    "\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "labels = [1, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5954922894424673"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "sum(data_test.danceability==prediction_labels)/len(prediction_labels)\n",
    "#end of CNN for danceability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
