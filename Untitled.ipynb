{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\JTOCo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1076 is causing problems\n",
      "row 2075 is causing problems\n",
      "row 2533 is causing problems\n",
      "row 3573 is causing problems\n",
      "row 4250 is causing problems\n",
      "row 4380 is causing problems\n",
      "row 4725 is causing problems\n",
      "row 4730 is causing problems\n",
      "row 5592 is causing problems\n",
      "row 5895 is causing problems\n",
      "row 6149 is causing problems\n",
      "row 6816 is causing problems\n",
      "row 7510 is causing problems\n",
      "row 7535 is causing problems\n",
      "499 have been removed.\n",
      "8424 songs remain in the dataset.\n",
      "stop words ['i', 'me', 'my', 'myself', 'we'] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lyric Data Processing\n",
    "CMPE 351 Group Project\n",
    "Spring 2021\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#%% Import actual data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ld = pd.read_csv('./data/track_features.csv')\n",
    "ld = ld[ld[\"lyrics\"]!=\"''\"]\n",
    "\n",
    "#%% Encode labels as 0 or 1\n",
    "\n",
    "ld.valence = round(ld.valence)\n",
    "ld.danceability = round(ld.danceability)\n",
    "\n",
    "#%% Language filter\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('words')\n",
    "def eng_ratio(text):\n",
    "    ''' Returns the ratio of non-English to English words from a text '''\n",
    "\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words()) \n",
    "    text_vocab = set(w.lower() for w in text.split() if w.lower().isalpha()) \n",
    "    unusual = text_vocab.difference(english_vocab)\n",
    "    diff = len(unusual)/len(text_vocab)\n",
    "    return diff\n",
    "\n",
    "\n",
    "before = ld.shape[0]\n",
    "for row_id in ld.index:\n",
    "    text = ld.loc[row_id]['lyrics']\n",
    "    try:\n",
    "        diff = eng_ratio(text)\n",
    "    except:\n",
    "        ld = ld[ld.index != row_id]\n",
    "        print('row %s is causing problems' %row_id)\n",
    "    if diff >= 0.5:\n",
    "        ld = ld[ld.index != row_id]\n",
    "after = ld.shape[0]\n",
    "rem = before - after\n",
    "print('%s have been removed.' %rem)\n",
    "print('%s songs remain in the dataset.' %after)\n",
    "\n",
    "dataPath1 = \"/Users/Ryan/Documents/GitHub/351-lyric-analysis/data/filtered_data.csv\"\n",
    "\n",
    "# ld.to_csv(os.path.join(dataPath1), index=False)\n",
    "\n",
    "#%% Split into training, test\n",
    "import numpy as np\n",
    "\n",
    "msk = np.random.rand(len(ld)) < 0.8\n",
    "\n",
    "train = ld[msk]\n",
    "test = ld[~msk]\n",
    "                 \n",
    "\n",
    "#%% Porter-Stemmer Tokenizer, suffix stripper\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
    "    \"\"\"\n",
    "    A Porter-Stemmer-Tokenizer hybrid to splits sentences into words (tokens) \n",
    "    and applies the porter stemming algorithm to each of the obtained token. \n",
    "    Tokens that are only consisting of punctuation characters are removed as well.\n",
    "    Only tokens that consist of more than one letter are being kept.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        \n",
    "    text : `str`. \n",
    "      A sentence that is to split into words.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    no_punct : `str`. \n",
    "      A list of tokens after stemming and removing Sentence punctuation patterns.\n",
    "    \n",
    "    \"\"\"\n",
    "    lower_txt = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(lower_txt)\n",
    "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
    "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return no_punct\n",
    "\n",
    "#%% Stop words\n",
    "\n",
    "# # One-time download of stop words file:\n",
    "# nltk.download('stopwords')\n",
    "# stp = nltk.corpus.stopwords.words('english')\n",
    "# with open('./stopwords_eng.txt', 'w') as outfile:\n",
    "#     outfile.write('\\n'.join(stp))\n",
    "    \n",
    "    \n",
    "with open('./stopwords_eng.txt', 'r') as infile:\n",
    "    stop_words = infile.read().splitlines()\n",
    "print('stop words %s ...' %stop_words[:5])\n",
    "\n",
    "#%% Count Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# can try different values for ngram_range\n",
    "countVec = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer,\n",
    "            ngram_range=(1,1)\n",
    "    )\n",
    "\n",
    "valenceTrain = train[\"valence\"]\n",
    "valenceTest = test[\"valence\"]\n",
    "danceTrain = train[\"danceability\"]\n",
    "danceTest = test[\"danceability\"]\n",
    "# print('Vocabulary size: %s' %len(countVecTrain.get_feature_names()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning of valence prediction\n",
    "valenceTrain = train[\"valence\"].astype(str)\n",
    "valenceTest = test[\"valence\"].astype(str)\n",
    "danceTrain = train[\"danceability\"].astype(str)\n",
    "danceTest = test[\"danceability\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1057\n",
       "0.0     626\n",
       "Name: valence, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#valenceTrain.value_counts()\n",
    "valenceTest.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0' '0.0' '0.0' ... '0.0' '0.0' '0.0']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-c0747c5abaca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNB_V_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mNB_V_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "print(NB_V_predictions)\n",
    "NB_V_predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[['0.0' '1.0']\n",
      " ['1282' '401']]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(NB_V_predictions, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='replace',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function porter_tokenizer at 0x0000021ADFF06438>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countVec.fit(train[\"lyrics\"].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVecTrain = countVec.transform(train[\"lyrics\"].values)\n",
    "countVecTest = countVec.transform(test[\"lyrics\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Valence Accuracy score: 0.5008912655971479\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Model \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(countVecTrain, valenceTrain)\n",
    "NB_V_predictions = clf_NB.predict(countVecTest)\n",
    "print('NB Valence Accuracy score:' , accuracy_score(valenceTest, NB_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Valence Accuracy: 0.5591206179441474\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf_RF=RandomForestClassifier(n_estimators=1)\n",
    "clf_RF.fit(countVecTrain,valenceTrain)\n",
    "RF_V_predictions = clf_RF.predict(countVecTest)\n",
    "print(\"RF Valence Accuracy:\", accuracy_score(valenceTest, RF_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Valence Accuracy: 0.6333927510398099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_LR = LogisticRegression()\n",
    "clf_LR.fit(countVecTrain, valenceTrain)\n",
    "NB_V_predictions = clf_LR.predict(countVecTest)\n",
    "print(\"LR Valence Accuracy:\", accuracy_score(valenceTest, NB_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JTOCo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JTOCo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-4cdb6a186eef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#Tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlower_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-4cdb6a186eef>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#Tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlower_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#put together dataset for CNN\n",
    "df = pd.read_csv ('filtered_data.csv')\n",
    "df1 = df[['lyrics', 'valence']]\n",
    "df1.valence = pd.cut(df1.valence,bins=[0,0.5,1],labels=[0,1])\n",
    "df1.lyrics = df1['lyrics'].astype(str)\n",
    "\n",
    "\n",
    "#Tokens\n",
    "tokens = [word_tokenize(sen) for sen in df1.lyrics]\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]\n",
    "\n",
    "#Pos and Neg\n",
    "pos = []\n",
    "neg = []\n",
    "for l in df1.valence:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "df1['Pos']= pos\n",
    "df1['Neg']= neg\n",
    "\n",
    "data = df1[['lyrics', 'tokens', 'valence', 'Pos', 'Neg']]\n",
    "data.head()\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "all_test_words = [word for tokens in data_test['tokens'] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test['tokens']]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print('%s words total, with a vocabulary size of %s' % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print('Max sentence length is %s' % max(test_sentence_lengths))\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)\n",
    "MS_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"lyrics\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"lyrics\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "tew = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    tew[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    \n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"lyrics\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "label_names = ['Pos', 'Neg']\n",
    "\n",
    "y_train = data_train[label_names].values\n",
    "\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n",
    "\n",
    "embedding_layer = Embedding(len(train_word_index)+1,\n",
    "                         300,\n",
    "                         weights=[tew],\n",
    "                         input_length=MS_LENGTH,\n",
    "                          trainable=False)\n",
    "    \n",
    "sequence_input = Input(shape=(MS_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "epochs_count = 3\n",
    "b_size = 36\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=250, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "\n",
    "lm = concatenate(convs, axis=1)\n",
    "\n",
    "x = Dropout(0.1)(lm)  \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(len(list(label_names)), activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(x_train, y_tr, epochs=epochs_count, validation_split=0.1, shuffle=True, batch_size=b_size)\n",
    "\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "labels = [1, 0]\n",
    "\n",
    "sum(data_test.valence==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5906120023767083"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble for Max voting for several models\n",
    "\"\"\"\n",
    "Addtional Models - SVM, DT\n",
    " \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "svc = SVC(kernel='poly, probability=True)\n",
    "dt = DecisionTreeClassifier()\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_estimators=1)\n",
    " \n",
    "classifiers = [('lr', lr), ('nb', nb), ('rf', rf)]\n",
    "vc = VotingClassifier(estimators=classifiers, voting='hard')\n",
    " \n",
    "# One method\n",
    "# from - https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ \n",
    "model = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "model.fit(countVecTrain, valenceTrain)\n",
    "model.score(countVecTest, valenceTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JTOCo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56920692 0.579937   0.54366981 0.58467601]\n"
     ]
    }
   ],
   "source": [
    "# Another method\n",
    "# Taken from - https://medium.com/@sanchitamangale12/voting-classifier-1be10db6d7a5 \n",
    "from sklearn.model_selection import cross_val_score\n",
    "a = []\n",
    "a.append(cross_val_score(lr, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    "a.append(cross_val_score(nb, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    "a.append(cross_val_score(rf, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    "a.append(cross_val_score(vc, countVecTest, valenceTest, scoring='accuracy', cv=5).mean())\n",
    " \n",
    "import numpy as np\n",
    "print(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things for CNN\n",
    "\n",
    "\"\"\"\n",
    "# Simple Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# replace line of code that\n",
    "# model = Model(sequence_input, preds)\n",
    "model = BaggingClassifier(Model(sequence_input, preds))\n",
    "\n",
    "# AdaBoostClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(Model(sequence_input, preds))\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "CNN Model - Average voting ensemble\n",
    "Adapted from https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "Josh's CNN model is used down below\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "# define Model + fit on dataset\n",
    "def fit_model(x_train, y_tr):\n",
    "    embedding_layer = Embedding(len(train_word_index)+1, 300, weights=[tew], input_length=MS_LENGTH, trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(MS_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    epochs_count = 5\n",
    "    b_size = 36\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=250, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    lm = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(lm)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(len(list(label_names)), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.fit(x_train, y_tr, epochs=epochs_count, validation_split=0.1, shuffle=True, batch_size=b_size)\n",
    "    return model\n",
    "\n",
    "# Ensemble prediction\n",
    "def ensemble_predictions(models, x_test):\n",
    "    # Make predictions\n",
    "    yhats = [model.predict(x_test) for model in models]\n",
    "    yhats = array(yhats)\n",
    "    # Sum ensemble members\n",
    "    model_sum = numpy.sum(yhats, axis=0)\n",
    "    result = argmax(model_sum, axis=1)\n",
    "    return result\n",
    "\n",
    "# Evaluate specifics\n",
    "# Double check names of testing/training data\n",
    "def evaluate_n_models(models, n_models, x_test, y_test):\n",
    "    subset = models[:n_models]\n",
    "    # Make a predicition\n",
    "    yhat = ensemble_predictions(subset, x_test)\n",
    "    return accuracy_score(test_y, yhat)\n",
    "\n",
    "n_models = 5\n",
    "models = [fit_model(train_x, train_y) for _ in range(n_models)]\n",
    "\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(models)+1):\n",
    "\t# evaluate model with i members\n",
    "\tensemble_score = evaluate_n_models(members, i, test_x, test_y)\n",
    "\t# evaluate the i'th model standalone\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, single_score = members[i-1].evaluate(test_x, testy_enc, verbose=0)\n",
    "\t# summarize this step\n",
    "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "\tensemble_scores.append(ensemble_score)\n",
    "\tsingle_scores.append(single_score)\n",
    "\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(models)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning of danceability prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Danceability Accuracy score: 0.477124183006536\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Model \n",
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(countVecTrain, danceTrain)\n",
    "NB_D_predictions = clf_NB.predict(countVecTest)\n",
    "print('NB Danceability Accuracy score:' , accuracy_score(danceTest, NB_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model\n",
    "clf_RF=RandomForestClassifier(n_estimators=100)\n",
    "clf_RF.fit(countVecTrain,danceTrain)\n",
    "RF_D_predictions = clf.predict(countVecTest)\n",
    "print(\"RF Danceability Accuracy:\", accuracy_score(danceTest, RF_V_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "clf_LR = LogisticRegression()\n",
    "clf_LR.fit(countVecTrain, danceTrain)\n",
    "NB_D_predictions = clf.predict(countVecTest)\n",
    "print(\"LR Danceability Accuracy:\", accuracy_score(danceTest, NB_V_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
